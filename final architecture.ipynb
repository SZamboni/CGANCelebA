{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia di FinalFinalAnal.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aGXiP0j_YHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup of the folder structure, following the guide:\n",
        "# https://medium.com/@opalkabert/downloading-kaggle-datasets-into-google-colab-fb9654c94235\n",
        "!pip install -U -q kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!kaggle datasets download -d jessicali9530/celeba-dataset \n",
        "!ls\n",
        "\n",
        "!cd /content\n",
        "!mkdir ./celeba\n",
        "!unzip /content/celeba-dataset.zip -d /content/celeba\n",
        "\n",
        "!mkdir /content/celeba/img_align_celeba/\n",
        "!unzip -q /content/celeba/img_align_celeba.zip -d /content/celeba/img_align_celeba/\n",
        "\n",
        "!cp /content/celeba/list_attr_celeba.csv /content/celeba/img_align_celeba/img_align_celeba/list_attr_celeba.csv "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwdWaG4pCHBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "  path = '/content/celeba/img_align_celeba/img_align_celeba/001901.jpg'\n",
        "  img = mpimg.imread(path)\n",
        "  imgplot = plt.imshow(img)\n",
        "  pd.read_csv('/content/celeba/img_align_celeba/img_align_celeba/list_attr_celeba.csv')\n",
        "  \n",
        "  \n",
        "  \n",
        "  print('main called')\n",
        "\n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2agqs29nvmWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Conncect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z51dAT15Bhh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Library Import\n",
        "\n",
        "from __future__ import print_function\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.nn import utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Define constants\n",
        "batch_size = 64 # at least 64\n",
        "num_epochs = 20\n",
        "input_dir = '/content/celeba/img_align_celeba/img_align_celeba/'\n",
        "output_dir = '/content/gdrive/My Drive/DeepLearningProject/Olivier Supreme Colab/Final1/'\n",
        "csv_name = 'list_attr_celeba.csv'\n",
        "num_workers = 2\n",
        "image_size = 64\n",
        "ngpu = 1\n",
        "g_input_dim = 100\n",
        "n_channels = 3 \n",
        "lr = 0.0002                 \n",
        "beta1 = 0.5\n",
        "visualization_step = 200\n",
        "n_labels = 2\n",
        "labels_number = [21,9] # Labels numers - 21: Male/Female, 9: Black hair/non Black hair\n",
        "\n",
        "ngf = 64                    # Size of feature maps in generator\n",
        "ndf = 64                    # Size of feature maps in discriminator\n",
        "\n",
        "'''\n",
        "Custom Dataset in order to load multiple labels from the csv containing labels\n",
        "'''\n",
        "class MyCustomDataset(torch.utils.data.Dataset):\n",
        "# __init__ function is where the initial logic happens like reading a csv\n",
        "# labels number should be an array of integers containing the number of the columns to take from the csv, like [19,13,8]\n",
        "    def __init__(self, input_dir, csv_name, image_size, labels_number):\n",
        "\n",
        "        # Create the transformation that will be applied: Resize, CenterCrop, ToTensor and Normalize\n",
        "        self.transform_to_apply = transforms.Compose([ transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size), transforms.ToTensor(), transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n",
        "\n",
        "        # Read the csv file\n",
        "        self.data_info = pd.read_csv(os.path.join(input_dir,csv_name))\n",
        "\n",
        "        # First column contains the image paths, second colstocazzoumn is the labels\n",
        "        self.image_arr = np.asarray(self.data_info.iloc[:, 0])\n",
        "\n",
        "        # Create the temporary array with the labels specified, initialize it at random\n",
        "        temp_labels = np.random.rand(self.image_arr.shape[0], len(labels_number))\n",
        "\n",
        "        # For every label that we want to use we load the corresponding csv coloumn in the temporary variable\n",
        "        for i in range(len(labels_number)):\n",
        "            temp_labels[0:-1,i] = (self.data_info.iloc[0:-1, labels_number[i] ] )\n",
        "\n",
        "        # Defenetively save the labels array, that will contains only the interesting labels\n",
        "        self.label_arr = np.asarray(temp_labels)\n",
        "\n",
        "        # Calculate lenght of the dataset\n",
        "        self.data_len = len(self.data_info.index)\n",
        "\n",
        "\n",
        "    # __getitem__ function returns the data and labels. This function is called from dataloader\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        # Get image name from the pandas df\n",
        "        single_image_name = self.image_arr[index]\n",
        "\n",
        "        # Open image and apply the transformation\n",
        "        img_as_img = Image.open(os.path.join(input_dir,single_image_name))\n",
        "        img = self.transform_to_apply(img_as_img)\n",
        "\n",
        "        # Get the labels of the image based on the cropped pandas column\n",
        "        single_image_label = self.label_arr[index]\n",
        "\n",
        "        # Normalize labels between 0 and 1\n",
        "        for i in range (single_image_label.size):\n",
        "            if single_image_label[i] < 0:\n",
        "                single_image_label[i] = 0\n",
        "\n",
        "        return (img, single_image_label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_len\n",
        "\n",
        "# Function that will create the dataset and the dataloader and return the dataloader\n",
        "def getDataLoader(input_dir,batch_size,image_size,num_workers,labels_number):\n",
        "\n",
        "    dataset = MyCustomDataset(input_dir,csv_name,image_size,labels_number)\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "\n",
        "    return data_loader\n",
        "\n",
        "# Conditional batch norm definition\n",
        "class CategoricalConditionalBatchNorm(torch.nn.Module):\n",
        "    # as in the chainer SN-GAN implementation, we keep per-cat weight and bias\n",
        "    def __init__(self, num_features, num_cats, eps=2e-5, momentum=0.1, affine=True,\n",
        "                 track_running_stats=True):\n",
        "        super().__init__()\n",
        "        self.num_features = num_features\n",
        "        self.num_cats = num_cats\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.affine = affine\n",
        "        self.track_running_stats = track_running_stats\n",
        "        if self.affine:\n",
        "            self.weight = torch.nn.Parameter(torch.Tensor(num_cats, num_features))\n",
        "            self.bias = torch.nn.Parameter(torch.Tensor(num_cats, num_features))\n",
        "        else:\n",
        "            self.register_parameter('weight', None)\n",
        "            self.register_parameter('bias', None)\n",
        "        if self.track_running_stats:\n",
        "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
        "            self.register_buffer('running_var', torch.ones(num_features))\n",
        "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
        "        else:\n",
        "            self.register_parameter('running_mean', None)\n",
        "            self.register_parameter('running_var', None)\n",
        "            self.register_parameter('num_batches_tracked', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_running_stats(self):\n",
        "        if self.track_running_stats:\n",
        "            self.running_mean.zero_()\n",
        "            self.running_var.fill_(1)\n",
        "            self.num_batches_tracked.zero_()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.reset_running_stats()\n",
        "        if self.affine:\n",
        "            self.weight.data.fill_(1.0)\n",
        "            self.bias.data.zero_()\n",
        "\n",
        "    def forward(self, input, cats):\n",
        "        exponential_average_factor = 0.0\n",
        "\n",
        "        if self.training and self.track_running_stats:\n",
        "            self.num_batches_tracked += 1\n",
        "            if self.momentum is None:  # use cumulative moving average\n",
        "                exponential_average_factor = 1.0 / self.num_batches_tracked.item()\n",
        "            else:  # use exponential moving average\n",
        "                exponential_average_factor = self.momentum\n",
        "\n",
        "        out = torch.nn.functional.batch_norm(\n",
        "            input, self.running_mean, self.running_var, None, None,\n",
        "            self.training or not self.track_running_stats,\n",
        "            exponential_average_factor, self.eps)\n",
        "        if self.affine:\n",
        "            shape = [input.size(0), self.num_features] + (input.dim() - 2) * [1]\n",
        "            weight = self.weight.index_select(0, cats).view(shape)\n",
        "            bias = self.bias.index_select(0, cats).view(shape)\n",
        "            out = out * weight + bias\n",
        "        return out\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return '{num_features}, num_cats={num_cats}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n",
        "               'track_running_stats={track_running_stats}'.format(**self.__dict__)\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "# G(z)\n",
        "class Generator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, num_classes, d=128):\n",
        "        super(Generator, self).__init__()\n",
        "        self.deconv1_1 = nn.ConvTranspose2d(100, d*4, 4, 1, 0)\n",
        "        self.deconv1_1_bn = nn.BatchNorm2d(d*4)\n",
        "        \n",
        "        self.deconv00_2 = nn.Conv2d(num_classes, int(d/4), 1, 1, 0)\n",
        "        self.deconv00_2_bn = nn.BatchNorm2d(int(d/4))\n",
        "        self.deconv0_2 = nn.ConvTranspose2d(int(d/4), d, 4, 1, 0)\n",
        "        self.deconv0_2_bn = nn.BatchNorm2d(d)\n",
        "        self.deconv1_2 = nn.ConvTranspose2d(d, d*4, 3, 1, 1)\n",
        "        self.deconv1_2_bn = nn.BatchNorm2d(d*4)\n",
        "        \n",
        "        self.deconv2 = nn.ConvTranspose2d(d*8, d*4, 4, 2, 1)\n",
        "        self.deconv2_bn = CategoricalConditionalBatchNorm(d*4,num_classes)\n",
        "        self.deconv3 = nn.ConvTranspose2d(d*4, d*2, 4, 2, 1)\n",
        "        self.deconv3_bn = CategoricalConditionalBatchNorm(d*2,num_classes)\n",
        "        self.deconv4 = nn.ConvTranspose2d(d*2, d, 4, 2, 1)\n",
        "        self.deconv4_bn = CategoricalConditionalBatchNorm(d,num_classes)\n",
        "        self.deconv5 = nn.ConvTranspose2d(d, 3, 4, 2, 1)\n",
        "\n",
        "    # weight_init\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "\n",
        "    # forward method\n",
        "    # def forward(self, input):\n",
        "    def forward(self, input, label, cat):\n",
        "        x = F.leaky_relu(self.deconv1_1_bn(self.deconv1_1(input)), 0.2)        \n",
        "        \n",
        "        y = F.leaky_relu(self.deconv00_2_bn(self.deconv00_2(label)), 0.2)\n",
        "        y = F.leaky_relu(self.deconv0_2_bn(self.deconv0_2(y)), 0.2)\n",
        "        y = F.leaky_relu(self.deconv1_2_bn(self.deconv1_2(y)), 0.2)        \n",
        "        \n",
        "        x = torch.cat([x, y], 1)\n",
        "        x = F.leaky_relu(self.deconv2_bn(self.deconv2(x),cat), 0.2)\n",
        "        x = F.leaky_relu(self.deconv3_bn(self.deconv3(x),cat), 0.2)\n",
        "        x = F.leaky_relu(self.deconv4_bn(self.deconv4(x),cat), 0.2)\n",
        "        x = torch.tanh(self.deconv5(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, d=128):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        #original: \n",
        "        self.conv1_1 = nn.Conv2d(3, int(d/2), 4, 2, 1)\n",
        "        self.conv0_2 = nn.Conv2d(2*n_labels, int(d/4), 1, 1, 0)\n",
        "        self.conv1_2 = nn.Conv2d(int(d/4), int(d/2), 4, 2, 1)\n",
        "        \n",
        "        #after union\n",
        "        self.conv2 = utils.spectral_norm(nn.Conv2d(d, d*2, 4, 2, 1))\n",
        "        self.conv3 = utils.spectral_norm(nn.Conv2d(d*2, d*4, 4, 2, 1))\n",
        "        self.conv4 = utils.spectral_norm(nn.Conv2d(d*4, d*8, 4, 2, 1))\n",
        "        self.conv5 = nn.Conv2d(d*8, 1, 4, 1, 0)\n",
        "\n",
        "    # weight_init\n",
        "    def weight_init(self, mean, std):\n",
        "        for m in self._modules:\n",
        "            normal_init(self._modules[m], mean, std)\n",
        "\n",
        "    # forward method\n",
        "    # def forward(self, input):\n",
        "    def forward(self, input, label):\n",
        "        #original: \n",
        "        x = F.leaky_relu(self.conv1_1(input), 0.2)\n",
        "        \n",
        "        y = F.leaky_relu(self.conv0_2(label), 0.2)\n",
        "        y = F.leaky_relu(self.conv1_2(y), 0.2)\n",
        "        \n",
        "        x = torch.cat([x, y], 1) #Discriminator\n",
        "        \n",
        "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
        "        x = F.leaky_relu(self.conv4(x), 0.2)\n",
        "        x = torch.sigmoid(self.conv5(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "def normal_init(m, mean, std):\n",
        "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
        "        m.weight.data.normal_(mean, std)\n",
        "        m.bias.data.zero_()\n",
        "\n",
        "# function to get the labels as tensor for the discriminator from the real labels in a vector\n",
        "def getDiscriminatorLabels(lbl, batch_size, image_size):\n",
        "\n",
        "    if(lbl.shape[1] == 1):\n",
        "\n",
        "        a = torch.zeros([batch_size,2,image_size,image_size])\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            if lbl[i][0] == 1:\n",
        "                a[i,0,:,:] = 1\n",
        "            else :\n",
        "                a[i,1,:,:] = 1\n",
        "        \n",
        "        return a\n",
        "\n",
        "    elif (lbl.shape[1] == 2):\n",
        "        \n",
        "        a = torch.zeros([batch_size,4,image_size,image_size])\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "          if lbl[i][0] == 1:\n",
        "                a[i,0,:,:] = 1\n",
        "          else :\n",
        "                a[i,1,:,:] = 1\n",
        "          if lbl[i][1] == 1:\n",
        "                a[i,2,:,:] = 1\n",
        "          else :\n",
        "                a[i,3,:,:] = 1\n",
        "\n",
        "        return a\n",
        "    \n",
        "    else:\n",
        "        a = torch.zeros([batch_size,6,image_size,image_size])\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "          if lbl[i][0] == 1:\n",
        "                a[i,0,:,:] = 1\n",
        "          else :\n",
        "                a[i,1,:,:] = 1\n",
        "          if lbl[i][1] == 1:\n",
        "                a[i,2,:,:] = 1\n",
        "          else :\n",
        "                a[i,3,:,:] = 1\n",
        "          if lbl[i][2] == 1:\n",
        "                a[i,4,:,:] = 1\n",
        "          else :\n",
        "                a[i,5,:,:] = 1\n",
        "        \n",
        "        return a\n",
        "\n",
        "# function to get the labels as tensor for the generator from the random labels in a vector\n",
        "def getGeneratorLabels(lbl,batch_size):\n",
        "\n",
        "    if(lbl.shape[1] == 1) :\n",
        "\n",
        "        a = torch.zeros([batch_size,2,1,1])\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            if lbl[i][0] == 1:\n",
        "                a[i,1,0,0] = 1\n",
        "            else:\n",
        "                a[i,0,0,0] = 1\n",
        "\n",
        "        return a \n",
        "\n",
        "    elif(lbl.shape[1] == 2):\n",
        "        a = torch.zeros([batch_size,4,1,1])\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            if lbl[i][0] == 1:\n",
        "                a[i,1,0,0] = 1\n",
        "            else:\n",
        "                a[i,0,0,0] = 1\n",
        "            if lbl[i][1] == 1:\n",
        "                a[i,3,0,0] == 1\n",
        "            else:\n",
        "                a[i,2,0,0] == 1\n",
        "        \n",
        "        return a\n",
        "    else:\n",
        "        a = torch.zeros([batch_size,6,1,1])\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            if lbl[i][0] == 1:\n",
        "                a[i,1,0,0] = 1\n",
        "            else:\n",
        "                a[i,0,0,0] = 1\n",
        "            if lbl[i][1] == 1:\n",
        "                a[i,3,0,0] == 1\n",
        "            else:\n",
        "                a[i,2,0,0] == 1\n",
        "            if lbl[i][2] == 1:\n",
        "                a[i,5,0,0] = 1\n",
        "            else:\n",
        "                a[i,4,0,0] = 1                \n",
        "                \n",
        "        return a\n",
        "\n",
        "# from the labels get the categories for the generator\n",
        "def getGeneratorCategories(lbl,batch_size):\n",
        "  \n",
        "    if(lbl.shape[1] == 1) :\n",
        "\n",
        "        a = torch.zeros([batch_size])\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            if lbl[i][0] == 1:\n",
        "                a[i] = 1\n",
        "            else:\n",
        "                a[i] = 0\n",
        "                \n",
        "        b = a.long()\n",
        "        return b\n",
        "\n",
        "    elif(lbl.shape[1] == 2):\n",
        "        a = torch.zeros([batch_size])\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            if lbl[i][0] == 0 and lbl[i][1] == 0:\n",
        "                a[i] = 0\n",
        "            elif lbl[i][0] == 0 and lbl[i][1] == 1:\n",
        "                a[i] = 1\n",
        "            elif lbl[i][0] == 0 and lbl[i][1] == 1:\n",
        "                a[i] = 2\n",
        "            else:\n",
        "                a[i] = 3\n",
        "            \n",
        "        b = a.long()\n",
        "        return b\n",
        "    else:\n",
        "        a = torch.zeros([batch_size])\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            if lbl[i][0] == 0 and lbl[i][1] == 0 and lbl[i][2] == 0:\n",
        "                a[i] = 0\n",
        "            elif lbl[i][0] == 0 and lbl[i][1] == 0 and lbl[i][2] == 1:\n",
        "                a[i] = 1\n",
        "            elif lbl[i][0] == 0 and lbl[i][1] == 1 and lbl[i][2] == 0:\n",
        "                a[i] = 2\n",
        "            elif lbl[i][0] == 0 and lbl[i][1] == 1 and lbl[i][2] == 1:\n",
        "                a[i] = 3\n",
        "            elif lbl[i][0] == 1 and lbl[i][1] == 0 and lbl[i][2] == 0:\n",
        "                a[i] = 4\n",
        "            elif lbl[i][0] == 1 and lbl[i][1] == 0 and lbl[i][2] == 1:\n",
        "                a[i] = 5\n",
        "            elif lbl[i][0] == 1 and lbl[i][1] == 1 and lbl[i][2] == 0:\n",
        "                a[i] = 6\n",
        "            else:\n",
        "                a[i] = 7\n",
        "        \n",
        "        b = a.long()\n",
        "        return b\n",
        "\n",
        "# get the generator labels in a tensor for the generator in order to have a grid of 8x8\n",
        "# generated images with alternatig labels      \n",
        "def getGeneratorVisualizationLabels(n_features,batch_size):\n",
        "    remaining = batch_size - 64\n",
        "    \n",
        "    if n_features == 1:\n",
        "        \n",
        "        zeros = np.zeros((32,1))\n",
        "        ones = np.ones((32,1))\n",
        "\n",
        "        if remaining > 0:\n",
        "            lbl_tmp = np.concatenate((zeros,ones),axis=0)\n",
        "            remaining_zeros = np.zeros((remaining,1))\n",
        "            lbl = np.concatenate((lbl_tmp,remaining_zeros),axis = 0)\n",
        "        else:\n",
        "            lbl = np.concatenate((zeros,ones),axis=0)\n",
        "\n",
        "        return lbl\n",
        "      \n",
        "    elif n_features == 2:\n",
        "        zeros = np.zeros((16,1))\n",
        "        ones = np.ones((16,1))\n",
        "        c1 = np.concatenate((zeros,ones,zeros,ones),axis =0)\n",
        "        c2 = np.concatenate((zeros,zeros,ones,ones),axis =0)\n",
        "        lbl = np.concatenate((c2,c1),axis=1)\n",
        "        \n",
        "        if(remaining >0):\n",
        "          remaining_zeros = np.zeros((remaining,2))\n",
        "          lbl = np.concatenate((lbl,remaining_zeros),axis = 0)\n",
        "        \n",
        "        return lbl\n",
        "      \n",
        "    else:\n",
        "        zeros = np.zeros((8,1))\n",
        "        ones = np.ones((8,1))\n",
        "        c1 = np.concatenate((zeros,ones,zeros,ones,zeros,ones,zeros,ones),axis =0)\n",
        "        c2 = np.concatenate((zeros,zeros,ones,ones,zeros,zeros,ones,ones),axis =0)\n",
        "        c3 = np.concatenate((zeros,zeros,zeros,zeros,ones,ones,ones,ones),axis =0)\n",
        "        lbl = np.concatenate((c3,c2,c1),axis=1)\n",
        "        \n",
        "        if(remaining >0):\n",
        "          remaining_zeros = np.zeros((remaining,3))\n",
        "          lbl = np.concatenate((lbl,remaining_zeros),axis = 0)\n",
        "        \n",
        "        return lbl\n",
        "\n",
        "\n",
        "# Main Program\n",
        "def main():\n",
        "    # Decide which device we want to run on\n",
        "    device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "    print(\"Use device: \" + str(device))\n",
        "    \n",
        "    tb = TensorBoardColab(graph_path='/log')\n",
        "    visualization_name='CELEBA-GAN-1'\n",
        "\n",
        "    # Load data\n",
        "    data_loader = getDataLoader(input_dir, batch_size, image_size, num_workers,labels_number)\n",
        "\n",
        "    # Save a sample of the training data\n",
        "    show_images = next(iter(data_loader))\n",
        "    plt.imshow(np.transpose(vutils.make_grid(show_images[0][0:64], padding=2, normalize=True),(1,2,0)))\n",
        "    plt.savefig(str(output_dir) + 'training_sample' + '.png')\n",
        "\n",
        "    # Create the Generator\n",
        "    netG = Generator(n_labels*2).to(device)\n",
        "    print(netG)\n",
        "\n",
        "    # Create the Discriminator\n",
        "    netD = Discriminator().to(device)\n",
        "    print(netD)\n",
        "\n",
        "    # Set the cost function\n",
        "    cost_fun = nn.BCELoss()\n",
        "\n",
        "    # I don't know why but in this order (net declaration, loss declaration and optimizer declaration)\n",
        "    # everything is working, maybe if you put the net declaration and immediately after the optimizer\n",
        "    # declaration something doesen't work\n",
        "\n",
        "    real_label = 1\n",
        "    fake_label = 0\n",
        "\n",
        "    # Create Discriminator and Generator optimizer\n",
        "    optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "    optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "    # Start traiting\n",
        "    for current_epoch in range(num_epochs):\n",
        "\n",
        "        for batch_index, (data,lbl) in enumerate(data_loader,0):\n",
        "            # Encode real labels for the discriminator\n",
        "            lbl_real_disc = getDiscriminatorLabels(lbl,batch_size, image_size)\n",
        "\n",
        "            # Reset discriminator, the same as optimizerD.zero_grad()\n",
        "            netD.zero_grad()\n",
        "            # Train discriminator on real data\n",
        "            real_data = data.to(device) #load the real data in the data variable\n",
        "            lbl_real_disc = lbl_real_disc.to(device)\n",
        "            # create a tensor of lenght batch size, with inside all cells the label indicating real data\n",
        "            b_size = real_data.size(0)\n",
        "            targets = torch.full((b_size,), real_label, device=device) \n",
        "            # compute the output of the discriminator, that will be a number between 0 and 1 for every batch element\n",
        "            outputs = netD(real_data,lbl_real_disc).view(-1)\n",
        "            # Compute the loss of the discriminator\n",
        "            real_loss = cost_fun(outputs,targets)\n",
        "            # Backward propagation for the discriminator\n",
        "            real_loss.backward()  \n",
        "            D_x = outputs.mean().item() \n",
        "\n",
        "            # Get random labels (array of 1 or 0)\n",
        "            fake_in_lbl_clear = np.random.randint(2, size=(batch_size,n_labels)) \n",
        "\n",
        "            # From the rangom labels get the labels for the generator\n",
        "            fake_in_lbl_g = getGeneratorLabels(fake_in_lbl_clear,batch_size)\n",
        "            fake_in_lbl_g = fake_in_lbl_g.to(device)\n",
        "\n",
        "            # From the rangom labels get the labels for the discriminator\n",
        "            fake_in_lbl_d = getDiscriminatorLabels(fake_in_lbl_clear,batch_size,image_size)\n",
        "            fake_in_lbl_d = fake_in_lbl_d.to(device)\n",
        "            \n",
        "            fake_cat = getGeneratorCategories(fake_in_lbl_clear,batch_size)\n",
        "            fake_cat = fake_cat.to(device)\n",
        "            \n",
        "            ## Train with all-fake batch\n",
        "            # Generate batch of latent vectors\n",
        "            noise = torch.randn(b_size, g_input_dim, 1, 1, device=device)\n",
        "            # Generate fake image batch with G\n",
        "            fake = netG(noise,fake_in_lbl_g,fake_cat)\n",
        "            targets.fill_(fake_label)\n",
        "            # Classify all fake batch with D\n",
        "            output = netD(fake.detach(),fake_in_lbl_d).view(-1)\n",
        "            # Calculate D's loss on the all-fake batch\n",
        "            errD_fake = cost_fun(output, targets)\n",
        "            # Calculate the gradients for this batch\n",
        "            errD_fake.backward()\n",
        "            D_G_z1 = output.mean().item()\n",
        "            # Add the gradients from the all-real and all-fake batches\n",
        "            errD = errD_fake\n",
        "            # Update D\n",
        "            optimizerD.step()\n",
        "        \n",
        "            # Train the generator\n",
        "            netG.zero_grad()\n",
        "            # fake labels are real for the generator, because the objective of the discriminator \n",
        "            # is to generate data that seems real, so that is labeled real\n",
        "            targets.fill_(real_label)\n",
        "            # Reuse fake data to make the discriminator classify again\n",
        "            outputs = netD(fake,fake_in_lbl_d).view(-1)\n",
        "            # The loss of the generator is the difference between the output of the discriminator and the real label\n",
        "            loss_g = cost_fun(outputs,targets)\n",
        "            # Generator backpropagation\n",
        "            loss_g.backward()\n",
        "            D_G_z2 = outputs.mean().item()\n",
        "            # Generator optimizer step\n",
        "            optimizerG.step()\n",
        "\n",
        "            if batch_index % visualization_step == 0:\n",
        "                print('Epoch ' + str(current_epoch) + '/' + str(num_epochs) + ' batch ' + str(batch_index) + '/' + str(len(data_loader)))\n",
        "\n",
        "                fake_vis_label_clear = getGeneratorVisualizationLabels(n_labels,batch_size)\n",
        "                fake_vis_label = getGeneratorLabels(fake_vis_label_clear,batch_size)\n",
        "                fake_vis_label = fake_vis_label.to(device)\n",
        "                fake_vis_cat = getGeneratorCategories(fake_vis_label_clear,batch_size)\n",
        "                fake_vis_cat = fake_vis_cat.to(device)\n",
        "                \n",
        "                noise_vis = torch.randn(b_size, g_input_dim, 1, 1, device=device)\n",
        "                visualFake = netG(noise_vis,fake_vis_label,fake_vis_cat)\n",
        "                imgplot = plt.imshow(np.transpose(vutils.make_grid(visualFake[0:64].detach().cpu(), padding=2, normalize=True),(1,2,0)))\n",
        "                plt.savefig(str(output_dir) + 'result_' + str(current_epoch) + '_' + str(batch_index) + '.png')\n",
        "                plt.show()\n",
        "                \n",
        "                print('Loss_D_real: ' + str(D_x) + ' Loss_D_fake: ' + str(D_G_z1) + ' Loss G: ' + str(D_G_z2) )\n",
        "                \n",
        "                current_batch = int(current_epoch*len(data_loader) + batch_index)\n",
        "                \n",
        "                tb.save_value('Loss D real',visualization_name,current_batch,float(D_x))\n",
        "                tb.save_value('Loss D fake',visualization_name,current_batch,float(D_G_z1))\n",
        "                tb.save_value('Loss G',visualization_name,current_batch,float(D_G_z2))\n",
        "                \n",
        "                 # Update plots \n",
        "                tb.flush_line(visualization_name)\n",
        "          \n",
        "          \n",
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}